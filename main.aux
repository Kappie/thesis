\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Contents}{i}{dummy.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Intro}{{1}{1}{Introduction}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Similarity of data}{1}{section.4}}
\citation{li2009introduction}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Normalized compression distance}{2}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{2}{Normalized compression distance}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Foundations in Kolmogorov complexity}{2}{section.6}}
\citation{Bennett1998}
\citation{Li2004}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Information distance}{3}{subsection.7}}
\newlabel{nid}{{2.1}{3}{Information distance}{equation.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Approximating Kolmogorov complexity with a real-world compressor}{3}{section.9}}
\citation{Cilibrasi2005}
\newlabel{ncd}{{2.2}{4}{Approximating Kolmogorov complexity with a real-world compressor}{equation.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}A compressor does not exploit all regularity}{4}{subsection.11}}
\citation{GenBank}
\citation{Cao1998}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hierarchical clustering of data}{5}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{5}{Hierarchical clustering of data}{chapter.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Evolution of placental mammals}{5}{section.14}}
\citation{Cilibrasi2005}
\citation{Cilibrasi2011}
\citation{Cilibrasi2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Distance matrix}{6}{subsection.15}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Distance matrix of genome sequences of 24 animals. To obtain the $\text  {NCD}$, I used ruby-xz, the Ruby binding to compressor liblzma, with settings \texttt  {compression\relax $\@@underline {\hbox {{ }}}\mathsurround \z@ $\relax level = 9}, \texttt  {extreme = true}, \texttt  {check = :none}. \relax }}{6}{table.caption.16}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:distance_matrix}{{3.1}{6}{Distance matrix of genome sequences of 24 animals. To obtain the $\text {NCD}$, I used ruby-xz, the Ruby binding to compressor liblzma, with settings \texttt {compression\underline {{ }}level = 9}, \texttt {extreme = true}, \texttt {check = :none}. \relax }{table.caption.16}{}}
\citation{Cao1998}
\citation{Cilibrasi2005}
\citation{Cilibrasi2011}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Clustering method}{7}{subsection.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Phylogeny tree}{7}{subsection.18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Random correlated data}{7}{section.20}}
\citation{Cilibrasi2005}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Result of average linkage clustering of the distance matrix shown in table \ref  {table:distance_matrix}. The height at which clusters join is proportional to the distance between them. \relax }}{8}{figure.caption.19}}
\newlabel{figure:dendrogram_mammals}{{3.1}{8}{Result of average linkage clustering of the distance matrix shown in table \ref {table:distance_matrix}. The height at which clusters join is proportional to the distance between them. \relax }{figure.caption.19}{}}
\citation{ruby}
\citation{clojure}
\citation{ghc}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Single link clustering of seven 80 kB files containing random bytes, which are partially correlated. The height at which clusters join is the distance between them, according to the linkage criterion.\relax }}{9}{figure.caption.21}}
\newlabel{figure:7_artificial_files}{{3.2}{9}{Single link clustering of seven 80 kB files containing random bytes, which are partially correlated. The height at which clusters join is the distance between them, according to the linkage criterion.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Literature}{9}{section.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Single link clustering of twenty-two 80 kB files containing random bytes, which are partially correlated.\relax }}{10}{figure.caption.22}}
\newlabel{figure:22_artificial_files}{{3.3}{10}{Single link clustering of twenty-two 80 kB files containing random bytes, which are partially correlated.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Source files}{10}{section.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Average link clustering of eight books by english and american writers in utf-8 format.\relax }}{11}{figure.caption.24}}
\newlabel{figure:8_books}{{3.4}{11}{Average link clustering of eight books by english and american writers in utf-8 format.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Average link clustering of 30 source files. The comments and blank lines were stripped using the \emph  {cloc} utility.\relax }}{12}{figure.caption.26}}
\newlabel{figure:30_source_files}{{3.5}{12}{Average link clustering of 30 source files. The comments and blank lines were stripped using the \emph {cloc} utility.\relax }{figure.caption.26}{}}
\citation{Garfinkel2009}
\citation{Li2010}
\citation{Veenman2007}
\citation{Axelsson2010}
\citation{Roussev2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Classification}{13}{chapter.27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{13}{Classification}{chapter.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Classifying file fragments}{13}{section.28}}
\citation{Li2010}
\citation{Cilibrasi2007}
\citation{Cortes1995}
\citation{Burges1998}
\citation{Pedregosa2011}
\citation{Fan2014}
\citation{Chang2011}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Example feature vector for a .csv and a .jpg fragment. The first five anchors are .csv fragments, the last five are .jpg fragments. All fragments are randomly drawn from the Govdocs1 corpus.\relax }}{14}{table.caption.32}}
\newlabel{table:feature_vectors}{{4.1}{14}{Example feature vector for a .csv and a .jpg fragment. The first five anchors are .csv fragments, the last five are .jpg fragments. All fragments are randomly drawn from the Govdocs1 corpus.\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Feature extraction}{14}{subsection.29}}
\citation{Burges1998}
\citation{Garfinkel2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Support vector machine}{15}{subsection.33}}
\citation{Pedregosa2011}
\citation{Chih2008}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Number of 512-byte fragments per file type.\relax }}{16}{table.caption.36}}
\newlabel{table:number_of_fragments}{{4.2}{16}{Number of 512-byte fragments per file type.\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Preparing the data set}{16}{subsection.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Training the classifier}{16}{subsection.37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4.1}Scaling the feature vectors}{16}{subsubsection.38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4.2}Parameter estimation}{16}{subsubsection.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Experiments}{17}{subsection.40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5.1}Classifying .csv and .jpg fragments}{17}{subsubsection.41}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces  The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. After feature extraction, the support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{1}, 2^{2}, \dots  , 2^{9} \}$ and $\gamma \in \{2^{-8}, 2^{-7}, \dots  , 2^{0} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }}{17}{table.caption.42}}
\newlabel{table:csv_jpg_recall}{{4.3}{17}{The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. After feature extraction, the support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{1}, 2^{2}, \dots , 2^{9} \}$ and $\gamma \in \{2^{-8}, 2^{-7}, \dots , 2^{0} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5.2}Classifying .gz and .jpg fragments}{17}{subsubsection.43}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces  The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. The kernel is RBF, $C \in \{ 2^{-3}, 2^{2}, \dots  , 2^{9} \}$ and $\gamma \in \{2^{-8}, 2^{-7}, \dots  , 2^{4} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }}{18}{table.caption.44}}
\newlabel{table:gz_jpg_recall}{{4.4}{18}{The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. The kernel is RBF, $C \in \{ 2^{-3}, 2^{2}, \dots , 2^{9} \}$ and $\gamma \in \{2^{-8}, 2^{-7}, \dots , 2^{4} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5.3}Classifying .csv, .html, .jpg and .log}{18}{subsubsection.45}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces  The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. The kernel is RBF, $C \in \{ 2^{0}, 2^{1}, \dots  , 2^{5} \}$ and $\gamma \in \{2^{-7}, 2^{-6}, \dots  , 2^{-1} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }}{18}{table.caption.46}}
\newlabel{table:csv_html_jpg_log_recall}{{4.5}{18}{The results were acquired by averaging recall rates over five independent runs. In each run, 1000 distinct training fragments and 500 distinct test fragments per file type were selected. The kernel is RBF, $C \in \{ 2^{0}, 2^{1}, \dots , 2^{5} \}$ and $\gamma \in \{2^{-7}, 2^{-6}, \dots , 2^{-1} \}$, optimized for the training set with a two-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.46}{}}
\bibstyle{unsrtnat}
\bibdata{library}
\bibcite{li2009introduction}{{1}{2009}{{Li and Vit\'{a}nyi}}{{}}}
\bibcite{Bennett1998}{{2}{1998}{{Bennett et~al.}}{{Bennett, G\^{a}cs, Li, Vit\^{a}nyi, and Zurek}}}
\bibcite{Li2004}{{3}{2004}{{Li et~al.}}{{Li, Chen, Li, Ma, and Vitanyi}}}
\bibcite{Cilibrasi2005}{{4}{2005}{{Cilibrasi and Vit\'{a}nyi}}{{}}}
\bibcite{GenBank}{{5}{}{{Gen}}{{}}}
\bibcite{Cao1998}{{6}{1998}{{Cao et~al.}}{{Cao, Janke, Waddell, Westerman, Takenaka, Murata, Okada, P\"{a}\"{a}bo, and Hasegawa}}}
\bibcite{Cilibrasi2011}{{7}{2011}{{Cilibrasi and Vitnyi}}{{}}}
\bibcite{ruby}{{8}{}{{rub}}{{}}}
\bibcite{clojure}{{9}{}{{clo}}{{}}}
\bibcite{ghc}{{10}{}{{ghc}}{{}}}
\bibcite{Garfinkel2009}{{11}{2009}{{Garfinkel et~al.}}{{Garfinkel, Farrell, Roussev, and Dinolt}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{19}{dummy.47}}
\newlabel{Bibliography}{{2}{19}{Classifying .csv, .html, .jpg and .log}{dummy.47}{}}
\bibcite{Li2010}{{12}{2010}{{Li et~al.}}{{Li, Ong, Suganthan, and Thing}}}
\bibcite{Veenman2007}{{13}{2007}{{Veenman}}{{}}}
\bibcite{Axelsson2010}{{14}{2010}{{Axelsson}}{{}}}
\bibcite{Roussev2013}{{15}{2013}{{Roussev and Quates}}{{}}}
\bibcite{Cilibrasi2007}{{16}{2007}{{Cilibrasi}}{{}}}
\bibcite{Cortes1995}{{17}{1995}{{Cortes and Vapnik}}{{}}}
\bibcite{Burges1998}{{18}{1998}{{Burges}}{{}}}
\bibcite{Pedregosa2011}{{19}{2011}{{Pedregosa et~al.}}{{Pedregosa, Weiss, and Brucher}}}
\bibcite{Fan2014}{{20}{2014}{{Fan et~al.}}{{Fan, Wang, and Lin}}}
\bibcite{Chang2011}{{21}{2011}{{Chang and Lin}}{{}}}
\bibcite{Chih2008}{{22}{2008}{{Hsu et~al.}}{{Hsu, Chang, and Lin}}}
