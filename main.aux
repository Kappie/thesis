\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{dummy.1}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Contents}{ii}{dummy.2}}
\citation{Cao1998}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Intro}{{1}{1}{Introduction}{chapter.4}{}}
\citation{Cilibrasi2007}
\citation{Pedregosa2011}
\citation{li2009introduction}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Normalized compression distance}{3}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{3}{Normalized compression distance}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Foundations in Kolmogorov complexity}{3}{section.6}}
\citation{Bennett1998}
\citation{Li2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Kolmogorov complexity is not computable}{4}{subsection.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Information distance}{4}{subsection.16}}
\citation{Cilibrasi2005}
\citation{Cilibrasi2005}
\newlabel{nid}{{2.1}{5}{Information distance}{equation.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Approximating Kolmogorov complexity with a real-world compressor}{5}{section.18}}
\newlabel{ncd}{{2.2}{5}{Approximating Kolmogorov complexity with a real-world compressor}{equation.19}{}}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}A compressor does not exploit all regularity}{6}{subsection.24}}
\citation{CompLearn}
\citation{Cilibrasi2011}
\citation{Pedregosa2011}
\citation{Cilibrasi2005}
\citation{Cilibrasi2005}
\citation{Cilibrasi2011}
\citation{ClusteringGithub}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hierarchical clustering of data}{7}{chapter.26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{7}{Hierarchical clustering of data}{chapter.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{7}{section.27}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Evolution of placental mammals}{7}{section.28}}
\citation{GenBank}
\citation{Cao1998}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Distance matrix}{8}{subsection.29}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  The NCDs were calculated with \texttt  {lzma}, the Python binding to the LZMA compression algorithm, with the parameters shown in listing \ref  {lzma_params}.\relax }}{8}{table.caption.30}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:distance_matrix}{{3.1}{8}{The NCDs were calculated with \texttt {lzma}, the Python binding to the LZMA compression algorithm, with the parameters shown in listing \ref {lzma_params}.\relax }{table.caption.30}{}}
\citation{XzManpage}
\citation{Cilibrasi2005}
\citation{Cilibrasi2011}
\newlabel{lzma_params}{{3.1}{9}{Parameters for compression using the LZMA algorithm}{lstlisting.31}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Parameters for compression using the LZMA algorithm.}{9}{lstlisting.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Compression parameters}{9}{subsubsection.40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Comparison of clustering methods}{9}{subsection.41}}
\citation{Cilibrasi2011}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.1}Quartet tree method}{10}{subsubsection.42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.2}Agglomerative (hierarchical) clustering with a linkage criterion}{10}{subsubsection.43}}
\newlabel{R_clustering}{{3.2}{10}{R code for plotting a hierarchical clustering. \lstinline {method} can be set to \lstinline {"average"}, \lstinline {"single"} or \lstinline {"complete"}, among other options}{lstlisting.44}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}R code for plotting a hierarchical clustering. \lstinline {method} can be set to \lstinline {"average"}, \lstinline {"single"} or \lstinline {"complete"}, among other options.}{10}{lstlisting.44}}
\citation{Cilibrasi2005}
\citation{CompLearn}
\citation{Cilibrasi2005}
\citation{Cao1998}
\citation{Cilibrasi2011}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Phylogeny tree of 24 mammals}{11}{subsection.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Result of average linkage clustering of the distance matrix shown in table \ref  {table:distance_matrix}. The height at which clusters join is proportional to the distance between them. \relax }}{11}{figure.caption.49}}
\newlabel{figure:dendrogram_mammals}{{3.1}{11}{Result of average linkage clustering of the distance matrix shown in table \ref {table:distance_matrix}. The height at which clusters join is proportional to the distance between them. \relax }{figure.caption.49}{}}
\citation{Cilibrasi2005}
\citation{Cilibrasi2005}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Random correlated data}{12}{section.50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Literature}{12}{section.53}}
\citation{ruby}
\citation{clojure}
\citation{ghc}
\citation{Cilibrasi2005}
\citation{CompLearn}
\citation{Cilibrasi2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Single link clustering of seven 80 kB files containing random bytes, which are partially correlated. The height at which clusters join is the distance between them, according to the linkage criterion.\relax }}{13}{figure.caption.51}}
\newlabel{figure:7_artificial_files}{{3.2}{13}{Single link clustering of seven 80 kB files containing random bytes, which are partially correlated. The height at which clusters join is the distance between them, according to the linkage criterion.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Source files}{13}{section.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Single link clustering of twenty-two 80 kB files containing random bytes, which are partially correlated.\relax }}{14}{figure.caption.52}}
\newlabel{figure:22_artificial_files}{{3.3}{14}{Single link clustering of twenty-two 80 kB files containing random bytes, which are partially correlated.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{14}{section.57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Average link clustering of eight books by english and american writers in utf-8 format.\relax }}{15}{figure.caption.54}}
\newlabel{figure:8_books}{{3.4}{15}{Average link clustering of eight books by english and american writers in utf-8 format.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Average link clustering of 30 source files. The comments and blank lines were stripped using the \emph  {cloc} utility.\relax }}{16}{figure.caption.56}}
\newlabel{figure:30_source_files}{{3.5}{16}{Average link clustering of 30 source files. The comments and blank lines were stripped using the \emph {cloc} utility.\relax }{figure.caption.56}{}}
\citation{Cilibrasi2007}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Classification}{17}{chapter.58}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{17}{Classification}{chapter.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{17}{section.59}}
\citation{Pedregosa2011}
\citation{ClassificationGithub}
\citation{Garfinkel2009}
\citation{Li2010}
\citation{Veenman2007}
\citation{Axelsson2010}
\citation{Roussev2013}
\citation{Li2010}
\citation{Cilibrasi2007}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Classifying file fragments}{18}{section.60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Feature extraction}{18}{subsection.61}}
\citation{Cortes1995}
\citation{Burges1998}
\citation{Pedregosa2011}
\citation{Fan2014}
\citation{Chang2011}
\citation{Burges1998}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Example feature vector for a .csv and a .jpg fragment. The first five anchors are .csv fragments, the last five are .jpg fragments. All fragments are randomly drawn from the Govdocs1 corpus.\relax }}{19}{table.caption.64}}
\newlabel{table:feature_vectors}{{4.1}{19}{Example feature vector for a .csv and a .jpg fragment. The first five anchors are .csv fragments, the last five are .jpg fragments. All fragments are randomly drawn from the Govdocs1 corpus.\relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Support vector machine}{19}{subsection.65}}
\citation{Garfinkel2009}
\citation{Pedregosa2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Experimental setup}{20}{subsection.67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.1}Preparing the data set}{20}{subsubsection.68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.2}Validating the classifier}{20}{subsubsection.70}}
\citation{Chih2008}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Number of 512-byte fragments per file type.\relax }}{21}{table.caption.69}}
\newlabel{table:number_of_fragments}{{4.2}{21}{Number of 512-byte fragments per file type.\relax }{table.caption.69}{}}
\@writefile{toc}{\contentsline {paragraph}{Rescaling the feature vectors}{21}{section*.71}}
\@writefile{toc}{\contentsline {paragraph}{Parameter estimation}{21}{section*.72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Results}{21}{subsection.73}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.1}Classifying .csv and .jpg fragments}{21}{subsubsection.74}}
\citation{Lakoff1973}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces  The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots  , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots  , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }}{22}{table.caption.75}}
\newlabel{table:csv_jpg_recall}{{4.3}{22}{The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.75}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.2}Classifying .gz and .jpg fragments}{22}{subsubsection.76}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces  The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots  , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots  , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }}{22}{table.caption.77}}
\newlabel{table:gz_jpg_recall}{{4.4}{22}{The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.3}Classifying .csv, .html, .jpg and .log}{22}{subsubsection.78}}
\citation{Schler2005}
\citation{Mulac2001}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces  Precision and recall rates for two anchors per file type. The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots  , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots  , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }}{23}{table.caption.79}}
\newlabel{table:csv_html_jpg_log_recall}{{4.5}{23}{Precision and recall rates for two anchors per file type. The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.79}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces  Precision and recall rates for ten anchors per file type. The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots  , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots  , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }}{23}{table.caption.80}}
\newlabel{table:csv_html_jpg_log_recall_10_anchors}{{4.6}{23}{Precision and recall rates for ten anchors per file type. The results were acquired by averaging precision rates over five independent runs. In each run, the distinct anchors were drawn randomly from the dataset. Then, 3000 distinct fragments (different from the anchors) per file type were selected. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. All fragments are 512 bytes.\relax }{table.caption.80}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Classifying written text by author gender}{23}{section.81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Literature overview}{23}{subsection.82}}
\citation{Schler2005}
\citation{Mikros2013}
\citation{Schler2005}
\citation{Sarawgi2011}
\citation{Schler2005}
\citation{Mikros2013}
\citation{Schler2005}
\citation{utf8_utils}
\citation{ruby_cld}
\citation{google_cld}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Classifying blogs by author gender using the normalized compression distance}{24}{subsection.83}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.1}Cleaning the corpus}{24}{subsubsection.84}}
\citation{Cilibrasi2007}
\citation{Pedregosa2011}
\citation{Koppel2002}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.2}Results}{25}{subsubsection.85}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces  The anchor blogs were randomly drawn from the dataset. Then, 5000 blogs, different from the anchors, were selected for each gender. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots  , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots  , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. \relax }}{25}{table.caption.86}}
\newlabel{table:female_male_precision}{{4.7}{25}{The anchor blogs were randomly drawn from the dataset. Then, 5000 blogs, different from the anchors, were selected for each gender. The predictions were validated with five-fold cross validation. The support vector machine with an RBF kernel was trained by doing a grid search for $C \in \{ 2^{-2}, 2^{2}, \dots , 2^{8} \}$ and $\gamma \in \{2^{-9}, 2^{-7}, \dots , 2^{1} \}$, optimized for each of the five training folds with a three-fold cross validation. \relax }{table.caption.86}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{25}{section.87}}
\citation{CompLearn}
\citation{Cilibrasi2011}
\citation{Cilibrasi2007}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{27}{chapter.88}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Conclusion}{{5}{27}{Conclusion}{chapter.88}{}}
\bibstyle{unsrtnat}
\bibdata{library}
\bibcite{Cao1998}{{1}{1998}{{Cao et~al.}}{{Cao, Janke, Waddell, Westerman, Takenaka, Murata, Okada, P\"{a}\"{a}bo, and Hasegawa}}}
\bibcite{Cilibrasi2005}{{2}{2005}{{Cilibrasi and Vit\'{a}nyi}}{{}}}
\bibcite{Cilibrasi2007}{{3}{2007}{{Cilibrasi}}{{}}}
\bibcite{Pedregosa2011}{{4}{2011}{{Pedregosa et~al.}}{{Pedregosa, Weiss, and Brucher}}}
\bibcite{li2009introduction}{{5}{2009}{{Li and Vit\'{a}nyi}}{{}}}
\bibcite{Bennett1998}{{6}{1998}{{Bennett et~al.}}{{Bennett, G\^{a}cs, Li, Vit\^{a}nyi, and Zurek}}}
\bibcite{Li2004}{{7}{2004}{{Li et~al.}}{{Li, Chen, Li, Ma, and Vitanyi}}}
\bibcite{CompLearn}{{8}{2015}{{Cilibrasi}}{{}}}
\bibcite{Cilibrasi2011}{{9}{2011}{{Cilibrasi and Vit\'{a}nyi}}{{}}}
\bibcite{ClusteringGithub}{{10}{2015{}}{{Kapteijns}}{{}}}
\bibcite{GenBank}{{11}{}{{Gen}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{28}{dummy.89}}
\newlabel{Bibliography}{{3}{28}{Conclusion}{dummy.89}{}}
\bibcite{XzManpage}{{12}{2014}{{XzM}}{{}}}
\bibcite{ruby}{{13}{}{{rub}}{{}}}
\bibcite{clojure}{{14}{}{{clo}}{{}}}
\bibcite{ghc}{{15}{}{{ghc}}{{}}}
\bibcite{ClassificationGithub}{{16}{2015{}}{{Kapteijns}}{{}}}
\bibcite{Garfinkel2009}{{17}{2009}{{Garfinkel et~al.}}{{Garfinkel, Farrell, Roussev, and Dinolt}}}
\bibcite{Li2010}{{18}{2010}{{Li et~al.}}{{Li, Ong, Suganthan, and Thing}}}
\bibcite{Veenman2007}{{19}{2007}{{Veenman}}{{}}}
\bibcite{Axelsson2010}{{20}{2010}{{Axelsson}}{{}}}
\bibcite{Roussev2013}{{21}{2013}{{Roussev and Quates}}{{}}}
\bibcite{Cortes1995}{{22}{1995}{{Cortes and Vapnik}}{{}}}
\bibcite{Burges1998}{{23}{1998}{{Burges}}{{}}}
\bibcite{Fan2014}{{24}{2014}{{Fan et~al.}}{{Fan, Wang, and Lin}}}
\bibcite{Chang2011}{{25}{2011}{{Chang and Lin}}{{}}}
\bibcite{Chih2008}{{26}{2008}{{Hsu et~al.}}{{Hsu, Chang, and Lin}}}
\bibcite{Lakoff1973}{{27}{1973}{{Lakoff}}{{}}}
\bibcite{Schler2005}{{28}{2005}{{Schler et~al.}}{{Schler, Koppel, Argamon, and Pennebaker}}}
\bibcite{Mulac2001}{{29}{2001}{{Mulac et~al.}}{{Mulac, Bradac, and Gibbons}}}
\bibcite{Mikros2013}{{30}{2013}{{Mikros}}{{}}}
\bibcite{Sarawgi2011}{{31}{2011}{{Sarawgi et~al.}}{{Sarawgi, Gajulapalli, and Choi}}}
\bibcite{utf8_utils}{{32}{}{{Clarke}}{{}}}
\bibcite{ruby_cld}{{33}{}{{rub}}{{}}}
\bibcite{google_cld}{{34}{}{{Sites}}{{}}}
\bibcite{Koppel2002}{{35}{2002}{{Koppel}}{{}}}
