Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Li2010,
author = {Li, Q and Ong, a and Suganthan, P and Thing, V},
file = {:Users/geertkapteijns/Downloads/li\_ong.pdf:pdf},
journal = {Proceedings of the South African Information Security Multi-Conference (SAISMC 2010)},
keywords = {data classification,digital forensics,support vector machine},
title = {{A Novel Support Vector Machine Approach to High Entropy Data Fragment Classification}},
url = {http://www1.i2r.a-star.edu.sg/~vriz/Publications/WDFIA\_SAISMC2010\_SVM\_High\_Entropy\_Data\_Classification.pdf$\backslash$npapers2://publication/uuid/0A3CD98F-9AA4-4DAF-A2C7-95CB9E080FB4},
year = {2010}
}
@article{Cao1998,
abstract = {The phylogenetic relationship among primates, ferungulates (artiodactyls + cetaceans + perissodactyls + carnivores), and rodents was examined using proteins encoded by the H strand of mtDNA, with marsupials and monotremes as the outgroup. Trees estimated from individual proteins were compared in detail with the tree estimated from all 12 proteins (either concatenated or summing up log-likelihood scores for each gene). Although the overall evidence strongly suggests ((primates, ferungulates), rodents), the ND1 data clearly support another tree, ((primates, rodents), ferungulates). To clarify whether this contradiction is due to (1) a stochastic (sampling) error; (2) minor model-based errors (e.g., ignoring site rate variability), or (3) convergent and parallel evolution (specifically between either primates and rodents or ferungulates and the outgroup), the ND1 genes from many additional species of primates, rodents, other eutherian orders, and the outgroup (marsupials + monotremes) were sequenced. The phylogenetic analyses were extensive and aimed to eliminate the following artifacts as possible causes of the aberrant result: base composition biases, unequal site substitution rates, or the cumulative effects of both. Neither more sophisticated evolutionary analyses nor the addition of species changed the previous conclusion. That is, the statistical support for grouping rodents and primates to the exclusion of all other taxa fluctuates upward or downward in quite a tight range centered near 95\% confidence. These results and a site-by-site examination of the sequences clearly suggest that convergent or parallel evolution has occurred in ND1 between primates and rodents and/or between ferungulates and the outgroup. While the primate/rodent grouping is strange, ND1 also throws some interesting light on the relationships of some eutherian orders, marsupials, and montremes. In these parts of the tree, ND1 shows no apparent tendency for unexplained convergences.},
author = {Cao, Ying and Janke, Axel and Waddell, Peter J. and Westerman, Michael and Takenaka, Osamu and Murata, Shigenori and Okada, Norihiro and P\"{a}\"{a}bo, Svante and Hasegawa, Masami},
doi = {10.1007/PL00006389},
file = {:Users/geertkapteijns/Downloads/conflict\_among\_mitochondrial\_proteins.pdf:pdf},
issn = {00222844},
journal = {Journal of Molecular Evolution},
keywords = {Convergent evolution,Mammalian phylogeny,Maximumlikelihood method,Mitochondrial proteins,ND1,Trees of individual proteins},
number = {3},
pages = {307--322},
pmid = {9732458},
title = {{Conflict among individual mitochondrial proteins in resolving the phylogeny of eutherian orders}},
volume = {47},
year = {1998}
}
@article{Li2004,
abstract = {A new class of distances appropriate for measuring similarity relations between sequences, say one type of similarity per distance, is studied.We propose a new “normalized informa- tion distance,” based on the noncomputable notion ofKolmogorov complexity, and show that it is in this class and it minorizes every computable distance in the class (that is, it is universal in that it discovers all computable similarities).We demonstrate that it is a metric and call it the similarity metric. This theory forms the foun- dation for a new practical tool. To evidence generality and robust- ness, we give two distinctive applications in widely divergent areas using standard compression programs like gzip andGenCompress. First, we compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we fully automatically compute the language tree of 52 different languages.},
archivePrefix = {arXiv},
arxivId = {cs/0111054},
author = {Li, M. and Chen, X. and Li, X. and Ma, B. and Vitanyi, P.M.B.},
doi = {10.1109/TIT.2004.838101},
eprint = {0111054},
file = {:Users/geertkapteijns/Downloads/similarity\_metric.pdf:pdf},
isbn = {0-89871-538-5},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {12},
pages = {3250--3264},
primaryClass = {cs},
title = {{The Similarity Metric}},
volume = {50},
year = {2004}
}
@article{Carbonell1998,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Carbonell, J and Goldstein, J},
doi = {10.1145/290941.291025},
file = {:Users/geertkapteijns/Downloads/carbonell\_goldstein.pdf:pdf},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {335--336},
title = {{The use of MMR, diversity-based reranking for reordering documents and producing summaries}},
url = {papers2://publication/uuid/1FA33AEC-2C9E-4149-B740-02A7C6C24B93},
year = {1998}
}
@misc{clojure,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/a1bc0e0cfa7e0b6ebabcdd80e3f7d3f22ab50615.html:html},
title = {clojure @ github.com},
url = {https://github.com/clojure/clojure}
}
@article{Cilibrasi2005,
abstract = { We present a new method for clustering based on compression. The method does not use subject-specific features or background knowledge, and works as follows: First, we determine a parameter-free, universal, similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files (singly and in pairwise concatenation). Second, we apply a hierarchical clustering method. The NCD is not restricted to a specific application area, and works across application area boundaries. A theoretical precursor, the normalized information distance, co-developed by one of the authors, is provably optimal. However, the optimality comes at the price of using the noncomputable notion of Kolmogorov complexity. We propose axioms to capture the real-world setting, and show that the NCD approximates optimality. To extract a hierarchy of clusters from the distance matrix, we determine a dendrogram (ternary tree) by a new quartet method and a fast heuristic to implement it. The method is implemented and available as public software, and is robust under choice of different compressors. To substantiate our claims of universality and robustness, we report evidence of successful application in areas as diverse as genomics, virology, languages, literature, music, handwritten digits, astronomy, and combinations of objects from completely different domains, using statistical, dictionary, and block sorting compressors. In genomics, we presented new evidence for major questions in Mammalian evolution, based on whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta hypothesis against the Theria hypothesis.},
archivePrefix = {arXiv},
arxivId = {cs/0312044},
author = {Cilibrasi, Rudi and Vit\'{a}nyi, P. M B},
doi = {10.1109/TIT.2005.844059},
eprint = {0312044},
file = {:Users/geertkapteijns/Downloads/cluster.pdf:pdf},
isbn = {0-7803-7728-1},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Heterogenous data analysis,Hierarchical unsupervised clustering,Kolmogorov complexity,Normalized compression distance,Parameter-free data mining,Quartet tree method,Universal dissimilarity distance},
number = {4},
pages = {1523--1545},
primaryClass = {cs},
title = {{Clustering by compression}},
volume = {51},
year = {2005}
}
@article{Li2008,
author = {Li, Ming and Li, Ming and Ma, Bin and Ma, Bin},
doi = {10.1145/1458082.1458242},
file = {:Users/geertkapteijns/Downloads/information\_shared\_by\_many\_objects.pdf:pdf},
isbn = {9781595939913},
journal = {Word Journal Of The International Linguistic Association},
keywords = {2,22,25,28,29,30,31,32,34,35,38,a complete list of,applications,data mining,however,in many data mining,information references is in,kolmogorov complexity,text mining,the third edition of,we are more},
pages = {1213--1220},
title = {{Information Shared by Many Objects}},
year = {2008}
}
@misc{GenBank,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/0aa01af9101ad7f92eedf17301e3269ae811fb50.html:html},
title = {0aa01af9101ad7f92eedf17301e3269ae811fb50 @ www.ncbi.nlm.nih.gov},
url = {http://www.ncbi.nlm.nih.gov/genbank/}
}
@article{Garfinkel2009,
abstract = {Progress in computer forensics research has been limited by the lack of a standardized data sets-corpora-that are available for research purposes. We explain why corpora are needed to further forensic research, present a taxonomy for describing corpora, and announce the availability of several forensic data sets. © 2009 Digital Forensic Research Workshop.},
author = {Garfinkel, Simson and Farrell, Paul and Roussev, Vassil and Dinolt, George},
doi = {10.1016/j.diin.2009.06.016},
file = {:Users/geertkapteijns/Downloads/forensic\_corpora.pdf:pdf},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Corpora,Forensics,Human subjects research,Real data corpus,Realistic data},
number = {SUPPL.},
pages = {2--11},
title = {{Bringing science to digital forensics with standardized forensic corpora}},
volume = {6},
year = {2009}
}
@article{Bennett1998,
abstract = {While Kolmogorov (1965) complexity is the accepted absolute measure of information content in an individual finite object, a similarly absolute notion is needed for the information distance between two individual objects, for example, two pictures. We give several natural definitions of a universal information metric, based on length of shortest programs for either ordinary computations or reversible (dissipationless) computations. It turns out that these definitions are equivalent up to an additive logarithmic term. We show that the information distance is a universal cognitive similarity distance. We investigate the maximal correlation of the shortest programs involved, the maximal uncorrelation of programs (a generalization of the Slepian-Wolf theorem of classical information theory), and the density properties of the discrete metric spaces induced by the information distances. A related distance measures the amount of nonreversibility of a computation. Using the physical theory of reversible computation, we give an appropriate (universal, antisymmetric, and transitive) measure of the thermodynamic work required to transform one object in another object by the most efficient process. Information distance between individual objects is needed in pattern recognition where one wants to express effective notions of "pattern similarity" or "cognitive similarity" between individual objects and in thermodynamics of computation where one wants to analyze the energy dissipation of a computation from a particular input to a particular output.},
archivePrefix = {arXiv},
arxivId = {1006.3520},
author = {Bennett, Charles H. and G\^{a}cs, P\'{e}ter and Li, Ming and Vit\^{a}nyi, Paul M B and Zurek, Wojciech H.},
doi = {10.1109/18.681318},
eprint = {1006.3520},
file = {:Users/geertkapteijns/Downloads/information\_distance\_bennett.pdf:pdf},
isbn = {1-4244-0134-8},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Algorithmic information theory,Description complexity,Entropy,Heat dissipation,Information distance,Information metric,Irreversible computation,Kolmogorov complexity,Pattern recognition},
number = {4},
pages = {1407--1423},
title = {{Information distance}},
volume = {44},
year = {1998}
}
@misc{Vitanyi2000,
author = {Vitanyi, P and Li, Ming},
booktitle = {\ldots , and Kolmogorov Complexity," Submitted to: IEEE \ldots},
file = {:Users/geertkapteijns/Downloads/introduction\_to\_kolmogorov.pdf:pdf},
isbn = {0387339981, 9780387339986},
number = {2},
pages = {446--464},
title = {{Minimum description length induction}},
url = {http://ukpmc.ac.uk/abstract/CIT/619030},
volume = {46},
year = {2000}
}
@misc{ghc,
title = {{The Glasgow Haskell Compiler}},
url = {https://www.haskell.org/ghc/}
}
@article{Cilibrasi2011,
abstract = {The Minimum Quartet Tree Cost problem is to construct an optimal weight tree from the 3(n4) weighted quartet topologies on n objects, where optimality means that the summed weight of the embedded quartet topologies is optimal (so it can be the case that the optimal tree embeds all quartets as nonoptimal topologies). We present a Monte Carlo heuristic, based on randomized hill-climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. All this is implemented and available, as part of the CompLearn package. We compare performance and running time of the original and improved versions with those of UPGMA, BioNJ, and NJ, as implemented in the SplitsTree package on genomic data for which the latter are optimized. ?? 2010 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cs/0606048},
author = {Cilibrasi, Rudi L. and Vitnyi, P. M B},
doi = {10.1016/j.patcog.2010.08.033},
eprint = {0606048},
file = {:Users/geertkapteijns/Downloads/quartet\_tree.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Data and knowledge visualization,Global optimization,Hierarchical clustering,Monte Carlo method,Pattern matchingApplications,Pattern matchingClusteringAlgorithms/ Similarity m,Quartet tree,Randomized hill-climbing},
number = {3},
pages = {662--677},
primaryClass = {cs},
title = {{A Fast Quartet tree heuristic for hierarchical clustering}},
volume = {44},
year = {2011}
}
@book{li2009introduction,
author = {Li, Ming and Vit\'{a}nyi, Paul M B},
publisher = {Springer Science \& Business Media},
title = {{An introduction to Kolmogorov complexity and its applications}},
year = {2009}
}
@article{Vitanyi2011,
abstract = {Information distance is a parameter-free similarity measure based on compression, used in pattern recognition, data mining, phylogeny, clustering and classification. The notion of information distance is extended from pairs to multiples (finite lists). We study maximal overlap, metricity, universality, minimal overlap, additivity and normalized information distance in multiples. We use the theoretical notion of Kolmogorov complexity which for practical purposes is approximated by the length of the compressed version of the file involved, using a real-world compression program.},
archivePrefix = {arXiv},
arxivId = {0905.3347},
author = {Vit\'{a}nyi, Paul M B},
doi = {10.1109/TIT.2011.2110130},
eprint = {0905.3347},
file = {:Users/geertkapteijns/Downloads/vitanyi\_information\_distance\_in\_multiples.pdf:pdf},
isbn = {0018-9448 VO - 57},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Data mining,Kolmogorov complexity,information distance,multiples,pattern recognition,similarity},
number = {4},
pages = {2451--2456},
title = {{Information distance in multiples}},
volume = {57},
year = {2011}
}
@article{Axelsson2010,
abstract = {We have applied the generalised and universal distance measure NCD - Normalised Compression Distance - to the problem of determining the type of file fragments. To enable later comparison of the results, the algorithm was applied to fragments of a publicly available corpus of files. The NCD algorithm in conjunction with the k-nearest-neighbour (k ranging from one to ten) as the classification algorithm was applied to a random selection of circa 3000 512-byte file fragments from 28 different file types. This procedure was then repeated ten times. While the overall accuracy of the n-valued classification only improved the prior probability from approximately 3.5\% to circa 32-36\%, the classifier reached accuracies of circa 70\% for the most successful file types. A prototype of a file fragment classifier was then developed and evaluated on new set of data (from the same corpus). Some circa 3000 fragments were selected at random and the experiment repeated five times. This prototype classifier remained successful at classifying individual file types with accuracies ranging from only slightly lower than 70\% for the best class, down to similar accuracies as in the prior experiment. ?? 2010 Digital Forensic Research Workshop. Published by Elsevier Ltd. All rights reserved.},
author = {Axelsson, Stefan},
doi = {10.1016/j.diin.2010.05.004},
file = {:Users/geertkapteijns/Downloads/ncd\_as\_file\_classifier.pdf:pdf},
issn = {17422876},
journal = {Digital Investigation},
number = {SUPPL.},
pages = {0--7},
title = {{The normalised compression distance as a file fragment classifier}},
volume = {7},
year = {2010}
}
@misc{ruby,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/8a5825d3be0372004b4e5af6d999ff10863e352f.html:html},
title = {ruby @ github.com},
url = {https://github.com/ruby/ruby}
}
@article{Marcu2002,
abstract = {We present a joint probability model for statistical machine translation, which au- tomatically learns word and phrase equiv- alents from bilingual corpora. Transla- tions produced with parameters estimated using the joint model are more accu- rate than translations produced using IBM Model 4.},
author = {Marcu, Daniel and Wong, William},
file = {:Users/geertkapteijns/Downloads/marcu\_wong.pdf:pdf},
journal = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing},
number = {July},
pages = {133--139},
title = {{A Phrase-Based , Joint Probability Model for Statistical Machine Translation}},
year = {2002}
}
@article{Roussev2013,
author = {Roussev, Vassil and Quates, Candice},
file = {:Users/geertkapteijns/Downloads/roussev\_quates.pdf:pdf},
keywords = {cation,data encoding identi fi,file type classi fi},
pages = {69--77},
title = {{File fragment encoding classi fi cation d An empirical approach}},
volume = {10},
year = {2013}
}
