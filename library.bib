Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Axelsson2010,
abstract = {We have applied the generalised and universal distance measure NCD - Normalised Compression Distance - to the problem of determining the type of file fragments. To enable later comparison of the results, the algorithm was applied to fragments of a publicly available corpus of files. The NCD algorithm in conjunction with the k-nearest-neighbour (k ranging from one to ten) as the classification algorithm was applied to a random selection of circa 3000 512-byte file fragments from 28 different file types. This procedure was then repeated ten times. While the overall accuracy of the n-valued classification only improved the prior probability from approximately 3.5\% to circa 32-36\%, the classifier reached accuracies of circa 70\% for the most successful file types. A prototype of a file fragment classifier was then developed and evaluated on new set of data (from the same corpus). Some circa 3000 fragments were selected at random and the experiment repeated five times. This prototype classifier remained successful at classifying individual file types with accuracies ranging from only slightly lower than 70\% for the best class, down to similar accuracies as in the prior experiment. ?? 2010 Digital Forensic Research Workshop. Published by Elsevier Ltd. All rights reserved.},
author = {Axelsson, Stefan},
doi = {10.1016/j.diin.2010.05.004},
file = {:Users/geertkapteijns/Downloads/ncd\_as\_file\_classifier.pdf:pdf},
issn = {17422876},
journal = {Digital Investigation},
number = {SUPPL.},
pages = {0--7},
title = {{The normalised compression distance as a file fragment classifier}},
volume = {7},
year = {2010}
}
@article{Li2004,
abstract = {A new class of distances appropriate for measuring similarity relations between sequences, say one type of similarity per distance, is studied.We propose a new “normalized informa- tion distance,” based on the noncomputable notion ofKolmogorov complexity, and show that it is in this class and it minorizes every computable distance in the class (that is, it is universal in that it discovers all computable similarities).We demonstrate that it is a metric and call it the similarity metric. This theory forms the foun- dation for a new practical tool. To evidence generality and robust- ness, we give two distinctive applications in widely divergent areas using standard compression programs like gzip andGenCompress. First, we compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we fully automatically compute the language tree of 52 different languages.},
archivePrefix = {arXiv},
arxivId = {cs/0111054},
author = {Li, M. and Chen, X. and Li, X. and Ma, B. and Vitanyi, P.M.B.},
doi = {10.1109/TIT.2004.838101},
eprint = {0111054},
file = {:Users/geertkapteijns/Downloads/similarity\_metric.pdf:pdf},
isbn = {0-89871-538-5},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {12},
pages = {3250--3264},
primaryClass = {cs},
title = {{The Similarity Metric}},
volume = {50},
year = {2004}
}
@article{Roussev2013,
author = {Roussev, Vassil and Quates, Candice},
file = {:Users/geertkapteijns/Downloads/roussev\_quates.pdf:pdf},
keywords = {cation,data encoding identi fi,file type classi fi},
pages = {69--77},
title = {{File fragment encoding classi fi cation d An empirical approach}},
volume = {10},
year = {2013}
}
@article{Cilibrasi2005,
abstract = { We present a new method for clustering based on compression. The method does not use subject-specific features or background knowledge, and works as follows: First, we determine a parameter-free, universal, similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files (singly and in pairwise concatenation). Second, we apply a hierarchical clustering method. The NCD is not restricted to a specific application area, and works across application area boundaries. A theoretical precursor, the normalized information distance, co-developed by one of the authors, is provably optimal. However, the optimality comes at the price of using the noncomputable notion of Kolmogorov complexity. We propose axioms to capture the real-world setting, and show that the NCD approximates optimality. To extract a hierarchy of clusters from the distance matrix, we determine a dendrogram (ternary tree) by a new quartet method and a fast heuristic to implement it. The method is implemented and available as public software, and is robust under choice of different compressors. To substantiate our claims of universality and robustness, we report evidence of successful application in areas as diverse as genomics, virology, languages, literature, music, handwritten digits, astronomy, and combinations of objects from completely different domains, using statistical, dictionary, and block sorting compressors. In genomics, we presented new evidence for major questions in Mammalian evolution, based on whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta hypothesis against the Theria hypothesis.},
archivePrefix = {arXiv},
arxivId = {cs/0312044},
author = {Cilibrasi, Rudi and Vit\'{a}nyi, P. M B},
doi = {10.1109/TIT.2005.844059},
eprint = {0312044},
file = {:Users/geertkapteijns/Downloads/cluster.pdf:pdf},
isbn = {0-7803-7728-1},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Heterogenous data analysis,Hierarchical unsupervised clustering,Kolmogorov complexity,Normalized compression distance,Parameter-free data mining,Quartet tree method,Universal dissimilarity distance},
number = {4},
pages = {1523--1545},
primaryClass = {cs},
title = {{Clustering by compression}},
volume = {51},
year = {2005}
}
@article{Pedregosa2011,
author = {Pedregosa, Fabian and Weiss, Ron and Brucher, Matthieu},
file = {:Users/geertkapteijns/Downloads/scikit\_learn.pdf:pdf},
pages = {2825--2830},
title = {{Scikit-learn : Machine Learning in Python}},
volume = {12},
year = {2011}
}
@article{Veenman2007,
abstract = {File carving is the process of recovering files from a disk without the help of a file system. In forensics, it is a helpful tool in finding hidden or recently removed disk content. Known signatures in file headers and footers are especially useful in carving such files out, that is, from header until footer. However, this approach assumes that file clusters remain in order. In case of file fragmentation, file clusters can be disconnected and the order can even be disrupted such that straighforward carving will fail. In this paper, we focus on methods for classifying clusters into file types by using the statistics of the clusters. By not exploiting the possible embedded signatures, we generate evidence from a different source that can be integrated later on. We propose a set of characteristic features and use statistical pattern recognition to learn a supervised classification model for a range of relevant file types. We exploit the statistics of a restricted number of neighboring clusters (context) to improve classification performance. In the experiments we show that the proposed features indeed enable the differentation of clusters into file types. Moreover, for some file types the incorporation of cluster context improves the recognition performance significantly.},
author = {Veenman, Cor J.},
doi = {10.1109/IAS.2007.75},
file = {:Users/geertkapteijns/Downloads/veenman\_file\_carving.pdf:pdf},
isbn = {0769528767},
journal = {Proceedings - IAS 2007 3rd Internationl Symposium on Information Assurance and Security},
pages = {393--398},
title = {{Statistical disk cluster classification for file carving}},
year = {2007}
}
@misc{ruby,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/8a5825d3be0372004b4e5af6d999ff10863e352f.html:html},
title = {ruby @ github.com},
url = {https://github.com/ruby/ruby}
}
@article{Li2008,
author = {Li, Ming and Li, Ming and Ma, Bin and Ma, Bin},
doi = {10.1145/1458082.1458242},
file = {:Users/geertkapteijns/Downloads/information\_shared\_by\_many\_objects.pdf:pdf},
isbn = {9781595939913},
journal = {Word Journal Of The International Linguistic Association},
keywords = {2,22,25,28,29,30,31,32,34,35,38,a complete list of,applications,data mining,however,in many data mining,information references is in,kolmogorov complexity,text mining,the third edition of,we are more},
pages = {1213--1220},
title = {{Information Shared by Many Objects}},
year = {2008}
}
@article{Cao1998,
abstract = {The phylogenetic relationship among primates, ferungulates (artiodactyls + cetaceans + perissodactyls + carnivores), and rodents was examined using proteins encoded by the H strand of mtDNA, with marsupials and monotremes as the outgroup. Trees estimated from individual proteins were compared in detail with the tree estimated from all 12 proteins (either concatenated or summing up log-likelihood scores for each gene). Although the overall evidence strongly suggests ((primates, ferungulates), rodents), the ND1 data clearly support another tree, ((primates, rodents), ferungulates). To clarify whether this contradiction is due to (1) a stochastic (sampling) error; (2) minor model-based errors (e.g., ignoring site rate variability), or (3) convergent and parallel evolution (specifically between either primates and rodents or ferungulates and the outgroup), the ND1 genes from many additional species of primates, rodents, other eutherian orders, and the outgroup (marsupials + monotremes) were sequenced. The phylogenetic analyses were extensive and aimed to eliminate the following artifacts as possible causes of the aberrant result: base composition biases, unequal site substitution rates, or the cumulative effects of both. Neither more sophisticated evolutionary analyses nor the addition of species changed the previous conclusion. That is, the statistical support for grouping rodents and primates to the exclusion of all other taxa fluctuates upward or downward in quite a tight range centered near 95\% confidence. These results and a site-by-site examination of the sequences clearly suggest that convergent or parallel evolution has occurred in ND1 between primates and rodents and/or between ferungulates and the outgroup. While the primate/rodent grouping is strange, ND1 also throws some interesting light on the relationships of some eutherian orders, marsupials, and montremes. In these parts of the tree, ND1 shows no apparent tendency for unexplained convergences.},
author = {Cao, Ying and Janke, Axel and Waddell, Peter J. and Westerman, Michael and Takenaka, Osamu and Murata, Shigenori and Okada, Norihiro and P\"{a}\"{a}bo, Svante and Hasegawa, Masami},
doi = {10.1007/PL00006389},
file = {:Users/geertkapteijns/Downloads/conflict\_among\_mitochondrial\_proteins.pdf:pdf},
issn = {00222844},
journal = {Journal of Molecular Evolution},
keywords = {Convergent evolution,Mammalian phylogeny,Maximumlikelihood method,Mitochondrial proteins,ND1,Trees of individual proteins},
number = {3},
pages = {307--322},
pmid = {9732458},
title = {{Conflict among individual mitochondrial proteins in resolving the phylogeny of eutherian orders}},
volume = {47},
year = {1998}
}
@article{Marcu2002,
abstract = {We present a joint probability model for statistical machine translation, which au- tomatically learns word and phrase equiv- alents from bilingual corpora. Transla- tions produced with parameters estimated using the joint model are more accu- rate than translations produced using IBM Model 4.},
author = {Marcu, Daniel and Wong, William},
file = {:Users/geertkapteijns/Downloads/marcu\_wong.pdf:pdf},
journal = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing},
number = {July},
pages = {133--139},
title = {{A Phrase-Based , Joint Probability Model for Statistical Machine Translation}},
year = {2002}
}
@article{Chang2011,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail},
author = {Chang, Chih-chung and Lin, Chih-jen},
doi = {10.1145/1961189.1961199},
file = {:Users/geertkapteijns/Downloads/libsvm.pdf:pdf},
isbn = {2157-6904},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
pages = {1--39},
pmid = {371},
title = {{LIBSVM : A Library for Support Vector Machines}},
volume = {2},
year = {2011}
}
@misc{clojure,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/a1bc0e0cfa7e0b6ebabcdd80e3f7d3f22ab50615.html:html},
title = {clojure @ github.com},
url = {https://github.com/clojure/clojure}
}
@article{Bennett1998,
abstract = {While Kolmogorov (1965) complexity is the accepted absolute measure of information content in an individual finite object, a similarly absolute notion is needed for the information distance between two individual objects, for example, two pictures. We give several natural definitions of a universal information metric, based on length of shortest programs for either ordinary computations or reversible (dissipationless) computations. It turns out that these definitions are equivalent up to an additive logarithmic term. We show that the information distance is a universal cognitive similarity distance. We investigate the maximal correlation of the shortest programs involved, the maximal uncorrelation of programs (a generalization of the Slepian-Wolf theorem of classical information theory), and the density properties of the discrete metric spaces induced by the information distances. A related distance measures the amount of nonreversibility of a computation. Using the physical theory of reversible computation, we give an appropriate (universal, antisymmetric, and transitive) measure of the thermodynamic work required to transform one object in another object by the most efficient process. Information distance between individual objects is needed in pattern recognition where one wants to express effective notions of "pattern similarity" or "cognitive similarity" between individual objects and in thermodynamics of computation where one wants to analyze the energy dissipation of a computation from a particular input to a particular output.},
archivePrefix = {arXiv},
arxivId = {1006.3520},
author = {Bennett, Charles H. and G\^{a}cs, P\'{e}ter and Li, Ming and Vit\^{a}nyi, Paul M B and Zurek, Wojciech H.},
doi = {10.1109/18.681318},
eprint = {1006.3520},
file = {:Users/geertkapteijns/Downloads/information\_distance\_bennett.pdf:pdf},
isbn = {1-4244-0134-8},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Algorithmic information theory,Description complexity,Entropy,Heat dissipation,Information distance,Information metric,Irreversible computation,Kolmogorov complexity,Pattern recognition},
number = {4},
pages = {1407--1423},
title = {{Information distance}},
volume = {44},
year = {1998}
}
@article{Garfinkel2009,
abstract = {Progress in computer forensics research has been limited by the lack of a standardized data sets-corpora-that are available for research purposes. We explain why corpora are needed to further forensic research, present a taxonomy for describing corpora, and announce the availability of several forensic data sets. © 2009 Digital Forensic Research Workshop.},
author = {Garfinkel, Simson and Farrell, Paul and Roussev, Vassil and Dinolt, George},
doi = {10.1016/j.diin.2009.06.016},
file = {:Users/geertkapteijns/Downloads/forensic\_corpora.pdf:pdf},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Corpora,Forensics,Human subjects research,Real data corpus,Realistic data},
number = {SUPPL.},
pages = {2--11},
title = {{Bringing science to digital forensics with standardized forensic corpora}},
volume = {6},
year = {2009}
}
@article{Fan2014,
abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
author = {Fan, Rong-en and Wang, Xiang-rui and Lin, Chih-jen},
file = {:Users/geertkapteijns/Downloads/liblinear.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {large-scale linear classification,logistic regression,machine learning,open source,support vector machines},
number = {2008},
pages = {1871--1874},
title = {{LIBLINEAR : A Library for Large Linear Classification}},
volume = {9},
year = {2014}
}
@article{Li2010a,
author = {Li, Q and Ong, a and Suganthan, P and Thing, V},
file = {:Users/geertkapteijns/Downloads/li\_ong.pdf:pdf},
journal = {Proceedings of the South African Information Security Multi-Conference (SAISMC 2010)},
keywords = {data classification,digital forensics,support vector machine},
title = {{A Novel Support Vector Machine Approach to High Entropy Data Fragment Classification}},
url = {http://www1.i2r.a-star.edu.sg/~vriz/Publications/WDFIA\_SAISMC2010\_SVM\_High\_Entropy\_Data\_Classification.pdf$\backslash$npapers2://publication/uuid/0A3CD98F-9AA4-4DAF-A2C7-95CB9E080FB4},
year = {2010}
}
@misc{GenBank,
file = {:Users/geertkapteijns/Library/Application Support/Mendeley Desktop/Downloaded/0aa01af9101ad7f92eedf17301e3269ae811fb50.html:html},
title = {0aa01af9101ad7f92eedf17301e3269ae811fb50 @ www.ncbi.nlm.nih.gov},
url = {http://www.ncbi.nlm.nih.gov/genbank/}
}
@article{Cortes1995,
abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demon- strated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
file = {:Users/geertkapteijns/Downloads/cortes\_vapniks.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Chen2009,
abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
author = {Chen, Hai-Feng and Chen, Hai-Feng},
doi = {10.1111/j.1747-0285.2009.00840.x},
file = {:Users/geertkapteijns/Downloads/cortes\_vapnik95.pdf:pdf},
isbn = {0885-6125},
issn = {1747-0285},
journal = {Chemical biology \& drug design},
keywords = {att,com,corinna,efficient learning algorithms,neural,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
pages = {273--297},
pmid = {19549084},
title = {{In Silico Log P Prediction for a Large Data Set with Support Vector Machines, Radial Basis Neural Networks and Multiple Linear Regression.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19549084},
volume = {297},
year = {2009}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, Christopher J C},
doi = {10.1023/A:1009715923555},
eprint = {1111.6189v1},
file = {:Users/geertkapteijns/Downloads/svm\_tutorial2.pdf:pdf},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
pages = {121--167},
pmid = {5207842081938259593},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
volume = {2},
year = {1998}
}
@article{Vitanyi2011,
abstract = {Information distance is a parameter-free similarity measure based on compression, used in pattern recognition, data mining, phylogeny, clustering and classification. The notion of information distance is extended from pairs to multiples (finite lists). We study maximal overlap, metricity, universality, minimal overlap, additivity and normalized information distance in multiples. We use the theoretical notion of Kolmogorov complexity which for practical purposes is approximated by the length of the compressed version of the file involved, using a real-world compression program.},
archivePrefix = {arXiv},
arxivId = {0905.3347},
author = {Vit\'{a}nyi, Paul M B},
doi = {10.1109/TIT.2011.2110130},
eprint = {0905.3347},
file = {:Users/geertkapteijns/Downloads/vitanyi\_information\_distance\_in\_multiples.pdf:pdf},
isbn = {0018-9448 VO - 57},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Data mining,Kolmogorov complexity,information distance,multiples,pattern recognition,similarity},
number = {4},
pages = {2451--2456},
title = {{Information distance in multiples}},
volume = {57},
year = {2011}
}
@book{li2009introduction,
author = {Li, Ming and Vit\'{a}nyi, Paul M B},
publisher = {Springer Science \& Business Media},
title = {{An introduction to Kolmogorov complexity and its applications}},
year = {2009}
}
@article{Koppel2002,
abstract = {The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy. The same techniques can be used to determine if a document is fiction or non-fiction with approximately 98 per cent accuracy.},
author = {Koppel, M.},
doi = {10.1093/llc/17.4.401},
file = {:Users/geertkapteijns/Downloads/male-female-llc-final.pdf:pdf},
isbn = {0268-1145},
issn = {0268-1145},
journal = {Literary and Linguistic Computing},
number = {4},
pages = {401--412},
title = {{Automatically Categorizing Written Texts by Author Gender}},
url = {http://llc.oxfordjournals.org/content/17/4/401.short},
volume = {17},
year = {2002}
}
@article{Cilibrasi2007,
author = {Cilibrasi, Rl},
file = {:Users/geertkapteijns/Downloads/cilibrasi\_thesis.pdf:pdf},
title = {{Statistical inference through data compression}},
url = {http://www.narcis.nl/publication/RecordID/oai:uva.nl:217776},
year = {2007}
}
@article{Chih2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:Users/geertkapteijns/Downloads/svm\_tutorial.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@misc{ghc,
title = {{The Glasgow Haskell Compiler}},
url = {https://www.haskell.org/ghc/}
}
@article{Carbonell1998,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Carbonell, J and Goldstein, J},
doi = {10.1145/290941.291025},
file = {:Users/geertkapteijns/Downloads/carbonell\_goldstein.pdf:pdf},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {335--336},
title = {{The use of MMR, diversity-based reranking for reordering documents and producing summaries}},
url = {papers2://publication/uuid/1FA33AEC-2C9E-4149-B740-02A7C6C24B93},
year = {1998}
}
@article{Cilibrasi2011,
abstract = {The Minimum Quartet Tree Cost problem is to construct an optimal weight tree from the 3(n4) weighted quartet topologies on n objects, where optimality means that the summed weight of the embedded quartet topologies is optimal (so it can be the case that the optimal tree embeds all quartets as nonoptimal topologies). We present a Monte Carlo heuristic, based on randomized hill-climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. All this is implemented and available, as part of the CompLearn package. We compare performance and running time of the original and improved versions with those of UPGMA, BioNJ, and NJ, as implemented in the SplitsTree package on genomic data for which the latter are optimized. ?? 2010 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cs/0606048},
author = {Cilibrasi, Rudi L. and Vitnyi, P. M B},
doi = {10.1016/j.patcog.2010.08.033},
eprint = {0606048},
file = {:Users/geertkapteijns/Downloads/quartet\_tree.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Data and knowledge visualization,Global optimization,Hierarchical clustering,Monte Carlo method,Pattern matchingApplications,Pattern matchingClusteringAlgorithms/ Similarity m,Quartet tree,Randomized hill-climbing},
number = {3},
pages = {662--677},
primaryClass = {cs},
title = {{A Fast Quartet tree heuristic for hierarchical clustering}},
volume = {44},
year = {2011}
}
@article{Mikros2013,
abstract = {The aim of this study is to obtain authorship attribution and author’s gender identification in a corpus of blogs written in Modern Greek language. More specifically, the corpus used contains 20 bloggers equally divided by gender (10 males \& 10 females) with 50 blog posts from each author (1,000 posts in total and an overall size of 406,460 words). From this corpus we calculated a number of standard stylometric variables (e.g. word length statistics and various vocabulary “richness” indices) and 300 most frequent word and character n-grams (character and word uni- grams, bigrams, trigrams). Support Vector Machines (SVM) were trained on this data, and the author’s gender prediction accuracy in 10-fold cross-validation experiment reached 82.6\% accuracy, a result that is comparable to current state-of-the-art author profiling systems. Authorship attribution accuracy reached 85.4\%, an equally satisfy- ing result given the large number of candidate authors (n=20). Keywords:},
author = {Mikros, George K.},
file = {:Users/geertkapteijns/Downloads/AA\_and\_GI\_in\_Greek\_blogs\_Qualico12.pdf:pdf},
journal = {Methods and Applications of Quantitative Linguistics},
keywords = {author profiling,authorship attribution,blogs,gender identification,ing,machine learn-,stylometry,support vector machines},
pages = {21--32},
title = {{Authorship Attribution and Gender Identification in Greek Blogs}},
year = {2013}
}
@misc{Vitanyi2000,
author = {Vitanyi, P and Li, Ming},
booktitle = {\ldots , and Kolmogorov Complexity," Submitted to: IEEE \ldots},
file = {:Users/geertkapteijns/Downloads/introduction\_to\_kolmogorov.pdf:pdf},
isbn = {0387339981, 9780387339986},
number = {2},
pages = {446--464},
title = {{Minimum description length induction}},
url = {http://ukpmc.ac.uk/abstract/CIT/619030},
volume = {46},
year = {2000}
}
@article{Li2010,
author = {Li, Q and Ong, a and Suganthan, P and Thing, V},
file = {:Users/geertkapteijns/Downloads/li\_ong.pdf:pdf},
journal = {Proceedings of the South African Information Security Multi-Conference (SAISMC 2010)},
keywords = {data classification,digital forensics,support vector machine},
title = {{A Novel Support Vector Machine Approach to High Entropy Data Fragment Classification}},
url = {http://www1.i2r.a-star.edu.sg/~vriz/Publications/WDFIA\_SAISMC2010\_SVM\_High\_Entropy\_Data\_Classification.pdf$\backslash$npapers2://publication/uuid/0A3CD98F-9AA4-4DAF-A2C7-95CB9E080FB4},
year = {2010}
}
