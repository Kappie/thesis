% Chapter Template

\chapter{Classification} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Classification}}

\section{Introduction}

Classification is a fundamental learning task. Given a training
set of objects for which the class is known, we want to identify the class
of an unknown object.

For example, we want to know if an incoming mail is \emph{spam}
(unsolicited message sent in bulk) or \emph{ham} (genuine message.) We
need an algorithm that extracts certain features from the incoming
message, compares it to features extracted from, say, a million messages
labeled as spam and a million labeled as ham, and decides to which class
it belongs.

Most real-world applications take a domain specific approach. Spam filters
count occurences of words and hyperlinks.

In this chapter, the normalized compression distance \eqref{ncd} is used
to extract features from objects. No domain knowledge is used. This method of feature extraction is hinted at in \cite{Cilibrasi2007}, but has, to my knowledge, never been used in experiment.

To classify a new piece of data, one determines how well it compresses with \emph{anchors}, preselected pieces of data from each class. The distances with these anchors are the features representing an object. With these features, a support vector machine can be trained. This support vector machine can then classify new objects.

First, I classify file fragments of different file types (for example .jpg or .xml), a common forensics task. This is a good sanity check for this method, because fragments should, in general, compress well with fragments of the same file type, and not well with fragments from different file types.

Then, I show that in gender attribution to written text, this method works almost as well as the state of the art, domain specific approaches, which use stylometric and content-related features.  Again, the code is light-weight: I only use methods from the Python machine learning library scikit-learn \cite{Pedregosa2011}.


\section{Classifying file fragments}

In forensics, file reconstruction is a big issue. Corrupted or wiped disks
may contain scattered fragments of files without metadata. File fragment
classification is a preliminary step in the reconstruction process.

The Govdocs1 dataset \cite{Garfinkel2009} is used throughout. It contains
nearly one million files of various types, acquired by querying search
egines with pseudo-random numbers and words.

Many authors, like \cite{Li2010}, \cite{Veenman2007} and
\cite{Axelsson2010}, try to classify compound file types (e.g. .doc or
.pdf.) Since a .ppt file may contain a .doc file, or a .pdf file may
contain a .jpg image, there is no way of telling the filetype of a 512 or
4096 byte block originating from such a file. If you misclassify .pdf as .jpg, did the classifier make a mistake, or did you stumble upon an embedded .jpg withing a .pdf? The answer is unknowable. This oversight is noted in \cite{Roussev2013}.

We will only use filetypes that are not compound, e.g. .html, .csv and
.jpg.

\subsection{Feature extraction}

Many statistical learning models, like artificial neural networks or
support vector machines, take as a training set fixed dimensional
vectors that correspond to a label:

\begin{equation}
  D = \{ (\vec{x_{i}}, y_{i}) \mid \vec{x_{i}} \in \mathbb{R}^{k}, i = 1, 2, \dots, n \}
\end{equation}

The mapping of actual objects onto feature vectors is the crucial step.
For example, in \cite{Li2010} byte frequencies are used as features, so
$\vec{x_{i}}$ is effectively a histogram, and $y_{i}$ is one of .dll,
.exe, .pdf, .mp3, .jpg.

In \cite{Cilibrasi2007} a method is described to extract features from
objects using the normalized compression distance. For each file type,
(randomly) select some \emph{anchors} from the corpus of file fragments.
For simplicity, let's assume a binary classification problem of fragments
that are either from a .jpg or a .csv file. After picking 5 anchor
fragments for each type (totaling 10), we calculate the feature vector $\vec{x}$ for a file $f$ as follows:

\begin{equation}\label{}
  \vec{x} = ( NCD(f, a_{1}), NCD(f, a_{2}), \dots, NCD(f, a_{10}) )
\end{equation}

Table \ref{table:feature_vectors} shows an example feature vector of
a fragment for both file types. It becomes clear why a fragment can be
characterized this way: it's easy to see that the first fragment is a .csv
file, since it is close to the first five anchors (the .csv anchors.) .jpg
is a compressed format, so it has a distance of 1 from both the .csv and
.jpg anchors.

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
 .csv & 0.91 & 0.94 & 0.94 & 0.86 & 0.95 & 1.01 & 1.00 & 1.01 & 1.02 & 1.02 \\
 .jpg & 1.01 & 1.01 & 1.01 & 1.01 & 1.01 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
\hline
\end{tabular} \caption{Example feature vector for a .csv and a .jpg
fragment. The first five anchors are .csv fragments, the last five are
.jpg fragments. All fragments are randomly drawn from the Govdocs1
corpus.} \label{table:feature_vectors} \end{table}

\subsection{Support vector machine}

A support vector machine is a trainable binary classifier\footnote{A
classifier for more than two classes can be created from a combination of
binary classifiers, for example with a one-vs-rest strategy.}, first introduced in its current form in
\cite{Cortes1995}. For an extensive introduction, see \cite{Burges1998}.
In all experiments, I used the Python library scikit-learn
\cite{Pedregosa2011}, which uses \cite{Fan2014} and \cite{Chang2011}
internally.

In short, we want to separate the data points in the $d$-dimensional
feature space with a hyperplane that maximizes the margin (Euclidian
distance) to the closest data points.

New data points fall on either side of the hyperplane and are classified
accordingly.

It is not possible to separate every set of points in all possible ways
with a linear function (i.e. a hyperplane.) You can show that hyperplanes
in $\mathbb{R}^{n}$ can, at most, separate $n + 1$ points in all possible
ways. So, if the training set is larger than the number of features plus
one, it may be inevitable that points in the training set are
misclassified by every hyperplane. The user-specified parameter $C$ weighs
the penalty for misclassified points in the training set. A higher value
of $C$ results in a hyperplane that misclassifies less training items.
(This may actually hurt the classifier's predictive power, a phenomenon
known as overfitting.)

With some mathematical footwork (see \cite{Burges1998}), the optimization
problem can be formulated in terms of only the dot products $\langle
\vec{x_{i}}, \vec{x_{j}} \rangle$ of feature vectors.

Nonlinear-classifiers can be created by replacing all dot products with
a \emph{kernel function} $k(\vec{x_{i}}, \vec{x_{j}})$. This effectively
applies a higher-dimensional transformation to the feature space.
A hyperplane is then constructed in that space, so that
non-separable data may become separable. This introduces extra parameters,
is more memory intensive, and is prone to overfitting, but often gives
better results.

Two common kernel functions are the polynomial one

$$ k(\vec{x_{i}}, \vec{x_{j}}) = ( \langle \vec{x_{i}}, \vec{x_{j}}
\rangle + r )^{d} $$

which introduces parameters $r$ and $d$, and the radial basis
function (RBF)

$$ k(\vec{x_{i}}, \vec{x_{j}}) = e^{-\gamma ||\vec{x_{i}}
- \vec{x_{j}}||^{2}} $$

which introduces $\gamma$.

\subsection{Experimental setup}

\subsubsection{Preparing the data set}

At the time of writing, it is possible to download samples of the Govdocs1
dataset \cite{Garfinkel2009}, called threads, of 1000 files each. I used
thread 0 through thread 7 as a corpus.

I created two corpora of fragmented files by chopping the files up into 512 or
4096 byte blocks, throwing away the first and last block of each file. The
     reasoning is that those blocks often contain header information, which is
     atypical for the file at large. It also ensures that all blocks are of
     equal size.

Table \ref{table:number_of_fragments} shows the number of fragments of each file type.

\begin{table}
\begin{tabular}{lr}
\hline
 File type   &   \# of fragments \\
\hline
 .csv        &                 47782 \\
 .jpg        &                 33630 \\
 .gz         &                 11507 \\
 .gif        &                 44367 \\
 .txt        &                 19539 \\
 .log        &                 25625 \\
 .xml        &                 9540  \\
 .html       &                 19934  \\
\hline
\end{tabular}
\caption{Number of 512-byte fragments per file type.}
\label{table:number_of_fragments}
\end{table}


\subsubsection{Validating the classifier}

In each experiment, $n_{\text{anchors}}$ anchors per class are randomly selected from
the dataset. From the remaining fragments, $n_{\text{items}}$ items per
class are randomly selected to extract features from. They will serve to
train and validate the classifier.

\paragraph{Rescaling the feature vectors} As is suggested in the
scikit-learn documentation (\cite{Pedregosa2011}), after we extract the
feature vectors from the file fragments (which contain NCDs mainly between
0.85 and 1.00), we rescale them to have zero mean and unit variance.

The classifier's effectiveness is assessed using $k$-fold cross
validation. The data set is partitioned into $k$ chunks of equal size. $k
- 1$ chunks are used as training data and the remaining chunk as test
data. In $k$ runs, every chunk is used once as test data. So, in the end,
every fragment in the data set is predicted once. For each training
partition, a grid search is performed to select the best parameters for
the model.


\paragraph{Parameter estimation}

In \cite{Chih2008}, the authors describe a practical method for training
a support vector machine, and show that a simple grid search on the
classifier's parameters can dramatically improve results.

When using a radial basis function kernel, for example, we need to find
the best combination of $C$ and $\gamma$. First, we try all combinations
$ \{ (C, \gamma) \mid C, \gamma \in 2^{n}, n = -10, -9, \dots, 9, 10 \}$.
Then, after the best $C$ and $\gamma$ in that grid have been determined,
we can perform successively more precise estimations by forming a finer
grid around the current optimum.

The fitness of the parameters is assessed by $k$-fold cross validation.
(So $k$-fold cross validation is used in two places -- to partition
the whole data set, and to find the best parameters for each of the
training partitions.)

\subsection{Results}

\subsubsection{Classifying .csv and .jpg fragments}

As a sanity check, we classify .csv and .jpg fragments. They should be
easily distinguishable, since .jpg has high entropy (it is a compressed
format) and .csv is very regular. Table \ref{table:csv_jpg_recall} shows
this to be the case. The precision is almost perfect.

\begin{table}[h]
\begin{tabular}{rrr}
\hline
   anchors/type &   .csv precision \% &   .jpg precision \% \\
\hline
              2 &           .9932 &           .9966 \\
              5 &           .9943 &           .9982 \\
              10 &          .9970 &           .9980 \\
\hline
\end{tabular}
\caption{
The results were acquired by averaging precision rates over five
independent runs. In each run, the distinct anchors were drawn randomly from the dataset.
Then, 3000 distinct fragments (different from the anchors) per file type were selected.
The predictions were validated with five-fold cross validation.
The support vector machine with an RBF kernel was trained by doing a grid search
for $C \in \{ 2^{-2}, 2^{2}, \dots, 2^{8} \}$ and $\gamma \in \{2^{-9},
2^{-7}, \dots, 2^{1} \}$, optimized for each of the five training folds with a three-fold
cross validation. All fragments are 512 bytes.}
\label{table:csv_jpg_recall}
\end{table}

\subsubsection{Classifying .gz and .jpg fragments}

Now, we train the classifier on two compressed formats. The recall rate is
very bad and .jpg gets many false positives. Training on .gif and .mp3
instead of .gz gives similar results. Increasing the training set from 1000 to 10000 sample fragments per type doesn't improve results, either.

This outcome is to be expected, because compressed file formats can hardly be compressed any further by lzma (or another compressor). They are effectively \emph{random} sequences, although they are, of course, not really random. Here, the difference between a real world compressor and the theoretical notion of the Kolmogorov complexity is especially clear: the (theoretical) information distance between two objects does not change if they are compressed, but the real-world equivalent, the normalized compression distance, does, because compressors cannot find the patterns in the already compressed data.

\begin{table}[h]
\begin{tabular}{rrrrr}
\hline
   anchors/type &   .gz precision &   .gz recall &   .jpg precision &   .jpg recall \\
\hline
              2 &           .6622 &           .7151 &    .7162 &    .4295 \\
             10 &           1     &           96.84 &    1     &    1     \\
\hline
\end{tabular}
\caption{
The results were acquired by averaging precision rates over five
independent runs. In each run, the distinct anchors were drawn randomly from the dataset.
Then, 3000 distinct fragments (different from the anchors) per file type were selected.
The predictions were validated with five-fold cross validation.
The support vector machine with an RBF kernel was trained by doing a grid search
for $C \in \{ 2^{-2}, 2^{2}, \dots, 2^{8} \}$ and $\gamma \in \{2^{-9},
2^{-7}, \dots, 2^{1} \}$, optimized for each of the five training folds with a three-fold
cross validation. All fragments are 512 bytes.} \label{table:gz_jpg_recall}
\end{table}



\subsubsection{Classifying .csv, .html, .jpg and .log}

Table \ref{table:csv_html_jpg_log_recall} shows that classification still
works very well on four different file types of low entropy. Note that
adding more anchors now significantly improves results.


\begin{table}[h]
\begin{tabular}{rrrrr}
\hline
   anchors/type &   .csv recall \% &   .html recall \% &   .jpg recall \% &   .log recall \% \\
\hline
              2 &           94.00    &            92.32 &           98.84 &           87.28 \\
             10 &           98.48 &            95.04 &           99.12 &           95.24 \\
\hline
\end{tabular}
\caption{
The results were acquired by averaging recall rates over five
independent runs. In each run, 1000 distinct training fragments and 500
distinct test fragments per file type were selected. The kernel is RBF, $C
\in \{ 2^{0}, 2^{1}, \dots, 2^{5} \}$ and $\gamma \in \{2^{-7}, 2^{-6},
\dots, 2^{-1} \}$, optimized for the training set with a two-fold cross
validation. All fragments are 512 bytes.}
\label{table:csv_html_jpg_log_recall}
\end{table}


\section{Classifying written text by author gender}

Men and women's language use differs, generally speaking. The
sociolinguistic essay \cite{Lakoff1973}, a commentary on women's place in
society, is regarded as the first research on this topic. The author
postulates there is men's language, like \emph{Shit, you've put the peanut
butter in the refridgerator again} and women's language, like \emph{Dear
me, did he kidnap the baby?} (examples taken from the article.)

The topic is fuzzy. It is not clear what patterns we are looking for.
That means the NCD is a good choice as a distance metric: we trust the compressor to find enough patterns, and do not input any domain-specific information ourselves.

\subsection{Literature overview}

In \cite{Schler2005}, the authors investigate a large number of blogs in
different age and gender brackets. They find big differences in style. Men
use more articles (\emph{a, an, the}) and prepositions (words like
\emph{after, besides, toward}), while women use more pronouns (\emph{I,
you, she, \dots}) and words conveying assent and negation (\emph{never,
ever, no, yes, \dots}) These findings are supported by the empirical
survey \cite{Mulac2001}. The same features distinguish older bloggers
(more articles and prepositions) from younger ones (pronouns,
assent/negation.) There are also differences in content. Words like
\emph{linux, microsoft, gaming} predict male authorship, while words like
\emph{shopping, mom, cried} predict female authorship.

The authors of \cite{Schler2005} use 502 stylometric features and 1000
content-related features (i.e. the 1000 words with the highest predictive
power) to train a classification algorithm and obtain 80.1\% precision
with ten-fold cross validation.

In \cite{Mikros2013}, a similar approach (stylometric and feature words) is used, but the author confuses gender attribution with authorship attribution (a task that is much better understood.) The data set consists of 1000 blog posts, by 10 male authors and 10 female authors (50 posts per author). The classifier predicts the right gender 85.4\% of the time, but because there are so many posts of the same author in the data set, the results of this experiment cannot be compared to the results of \cite{Schler2005} and this paper.

Finally, \cite{Sarawgi2011} tries to predict gender based purely on writing style (as opposed to \cite{Schler2005} and \cite{Mikros2013} who also use content-related features.) The author tries to mitigate the gender-bias in topic and genre by hand selecting blogs posts with the same topic, e.g. the television show \emph{How I Met Your Mother}. This seems problematic to me: a human might unconsciously select blog posts that comply to her idea of male or female writing, thereby biasing the experiment. Also, the dataset is very small (280 posts, 140 per gender) and the author tries eight different feature-extraction methods, of which the best one yields 71.3\%. This result is hard to compare, because it is the best result of multiple hypotheses, whereas other papers used only one.

\subsection{Classifying blogs by author gender using the normalized compression distance}
\subsubsection{Cleaning the corpus}

I use the Blog Authorship Corpus, assembled by the authors of \cite{Schler2005}, which contains blogs of 19320 bloggers (averaging 35 posts per person). Blogs are labeled by author gender, age, and genre.

For each blog, I used only the plain text of each post, removing leading and trailing newlines. I tried to replace characters that were not utf-8 with comparable utf-8 alternatives using \lstinline{utf8_utils} \cite{utf8_utils}, a Ruby library. If this could not be done, I discarded the post. I automatically removed all posts that were not in English, using the Ruby binding \cite{ruby_cld} to the Google Chromium Compact Language Detector \cite{google_cld}. The detector also removed posts that only consisted of a hyperlink. I concatenated the remaining posts into a single file.

\subsubsection{Results}
Table \ref{table:female_male_precision} shows the results achieved by classifying based on the normalized compression distance. A 75\% recall rate is quite remarkable, considering no domain features at all were used. Many experiments using domain-specific features do not even reach this number, and the highest valid figure I saw in the literature is 80.1\%.

\begin{table}[h]
\begin{tabular}{rrr}
\hline
   anchors/type &   female recall \% &   male recall \% \\
\hline
            100 &             0.745 &          0.7551 \\
\hline
\end{tabular}
\caption{
The anchor blogs were randomly drawn from the dataset. Then, 5000 blogs, different from the anchors, were selected for each gender.
The predictions were validated with five-fold cross validation.
The support vector machine with an RBF kernel was trained by doing a grid search
for $C \in \{ 2^{-2}, 2^{2}, \dots, 2^{8} \}$ and $\gamma \in \{2^{-9},
2^{-7}, \dots, 2^{1} \}$, optimized for each of the five training folds with a three-fold
cross validation. }
\label{table:female_male_precision}
\end{table}

\subsection{Conclusion}

This chapter shows that the normalized compression distance can be used to extract features from data without using domain-specific knowledge. Those features can then be used to train a classifier (in this paper, a support vector machine) to predict the class of new data items. This method was briefly described in \cite{Cilibrasi2007}, but has, to my knowledge, never been used in experiment. The Python script to run experiments is small, and makes extensive use of the scikit-learn library \cite{Pedregosa2011}.

In the field of file fragment classification. Bla

In the field of gender attribution, it is less clear what patterns to look for, and the NCD shines. In classifying the gender of the authors of blogs, my algorithm has a 75\% precision rate, which is very high, considering no domain specific features where extracted, and the highest number in literature is 80.1\%, on the same corpus I used.

As follow-up research, one could try to use a combined method: Using (a subset of) the stylometric and content-related features, along with NCDs with fifty or hundred anchors.

Other fields in which NCD could be applied are classification of fiction and non-fiction text, as is done in \cite{Koppel2002}, or other classification by genre, like news stories or financial messages.
